import os
os.environ["OPENAI_API_KEY"] = ""
os.environ["AZURE_OPENAI_ENDPOINT"] = ""
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_VERSION"] = "2023-05-15"

import requests

# Azure Computer Vision API ì„¤ì •
SUBSCRIPTION_KEY = ""
ENDPOINT = ""

from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import AzureChatOpenAI

def read_image_ocr(image_path):
    """ì´ë¯¸ì§€ë¥¼ Azure OCR APIì— ì „ì†¡í•˜ê³  ê²°ê³¼ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    ocr_url = ENDPOINT + "vision/v3.2/ocr"
    headers = {
        'Ocp-Apim-Subscription-Key': SUBSCRIPTION_KEY,
        'Content-Type': 'application/octet-stream'
    }
    try:
        with open(image_path, 'rb') as image:
            image_data = image.read()
    except Exception as e:
        return f" ì´ë¯¸ì§€ ì½ê¸° ì˜¤ë¥˜: {e}"

    response = requests.post(ocr_url, headers=headers, data=image_data)
    if response.status_code != 200:
        return f" OCR ìš”ì²­ ì‹¤íŒ¨: {response.status_code}, {response.text}"

    return response.json()

def extract_ocr_text(ocr_result):
    """OCR JSON ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    text = ""
    for region in ocr_result.get("regions", []):
        for line in region.get("lines", []):
            line_text = " ".join(word.get("text", "") for word in line.get("words", []))
            text += line_text + "\n"
    return text.strip()

# ì´ë¯¸ì§€ ì—…ë¡œë“œ ì…€
image_path = input("ì´ë¯¸ì§€ì˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")

ocr_result = read_image_ocr(image_path)
if not isinstance(ocr_result, dict):
  print(ocr_result)

extracted_text = extract_ocr_text(ocr_result)
print("\n ì¶”ì¶œëœ í…ìŠ¤íŠ¸:\n")
print(extracted_text)


# ëª¨ë¸ ì„¤ì •
llm = AzureChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
chat_prompt = ChatPromptTemplate.from_template(
    "ë‚´ê°€ ëª»ë¨¹ëŠ” ì¬ë£ŒëŠ” {inputA}ì¸ë° {topic} ì´ì¤‘ì— ë‚´ê°€ ëª»ë¨¹ëŠ” ì¬ë£Œê°€ í¬í•¨ë˜ì–´ìˆìœ¼ë©´ ì•Œë ¤ì¤˜"
)

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
inputA = input("ëª» ë¨¹ëŠ” ì¬ë£Œë“¤ì„ ì‰¼í‘œë¡œ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê³„ë€, ìš°ìœ ): ")
topic = extracted_text

# í”„ë¡¬í”„íŠ¸ ì™„ì„±
prompt_value = chat_prompt.format_prompt(inputA=inputA, topic=topic)

# ëª¨ë¸ì—ê²Œ ì§ˆë¬¸ ë³´ë‚´ê¸°
response = llm.invoke(prompt_value)

# ì²« ê²°ê³¼ ì €ì¥
previous_result = response.content

# ê²°ê³¼ ì¶œë ¥
print("\n ê²°ê³¼:")
print(previous_result)


# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìˆ˜ì •: ì´ì „ ê²°ê³¼ë¥¼ í¬í•¨í•œ ì‚¬ìš©ì ì§ˆë¬¸ ëŒ€ì‘
followup_prompt = ChatPromptTemplate.from_template(
    "ì´ì „ ëŒ€í™” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{previous}\n\nì‚¬ìš©ì ì§ˆë¬¸: {question}\në‹µë³€:"
)

# ì…ë ¥ê°’ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
user_input = input("\nğŸ’¬ ë¬´ì—‡ì´ ê¶ê¸ˆí•œê°€ìš”? ")

prompt_value = followup_prompt.format_prompt(
    previous=previous_result,
    question=user_input
)
# LLMì— ìš”ì²­
response = llm.invoke(prompt_value)

# ê²°ê³¼ ì¶œë ¥

print("\n ì´ì–´ì§€ëŠ” ë‹µë³€:")
print(response.content)

